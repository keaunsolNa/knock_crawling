import logging
import re
from typing import List
from bs4 import BeautifulSoup, ResultSet, Tag
from crawling.base.abstract_crawling_service import AbstractCrawlingService
from method.StringDateConvert import StringDateConvertLongTimeStamp
from infra.elasticsearch_config import get_es_client
from infra.es_utils import load_all_categories_into_cache, fetch_or_create_category, search_kofic_index_by_title_and_director, exists_movie_by_kofic_code
from crawling.base.webdriver_config import create_driver, scroll_until_loaded
from crawling.services.crawling_util import get_detail_data

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)
converter = StringDateConvertLongTimeStamp()

es = get_es_client()
load_all_categories_into_cache("MOVIE")

def extract_detail_url(element: Tag) -> str:
    onclick = element.select_one("a.btn_reserve").get("onclick", "")
    match = re.search(r"fnQuickReserve\('(\d+)'", onclick)
    if match:
        code = match.group(1)
        return f"https://www.cgv.co.kr/movies/detail-view/?midx={code}"
    logger.warning("[CGV] ì˜ˆë§¤ ì½”ë“œ ì¶”ì¶œ ì‹¤íŒ¨: %s", onclick)
    return ""

def extract_director_and_actors(soup: BeautifulSoup) -> (List[str], List[str]):
    spec_block = soup.select_one("div.spec")
    if not spec_block:
        return [], []

    directors, actors = [], []
    dl_children = spec_block.select_one("dl").find_all(["dt", "dd"], recursive=False)

    current_label = None
    for tag in dl_children:
        if tag.name == "dt":
            text = tag.get_text(strip=True).replace(" ", "").replace(":", "").replace("/", "")
            current_label = text  # ex: "ê°ë…", "ë°°ìš°"
        elif tag.name == "dd" and current_label:
            if "ê°ë…" in current_label:
                directors.extend([a.text.strip() for a in tag.select("a") if a.text.strip()])
            elif "ë°°ìš°" in current_label:
                # ë°°ìš°ê°€ a íƒœê·¸ì— ì—†ì„ ìˆ˜ë„ ìžˆìŒ (í…ìŠ¤íŠ¸ ë¶„ë¦¬)
                if tag.select("a"):
                    actors.extend([a.text.strip() for a in tag.select("a") if a.text.strip()])
                else:
                    raw_text = tag.get_text(separator=",").strip()
                    actors.extend([a.strip() for a in raw_text.split(",") if a.strip()])

    return directors, actors

def extract_genre(soup: BeautifulSoup) -> str:
    spec_block = soup.select_one("div.spec")
    if not spec_block:
        return "ê¸°íƒ€"

    dt_elements = spec_block.select("dt")

    for dt in dt_elements:
        if "ìž¥ë¥´" in dt.text:
            genre_text = dt.get_text(strip=True).replace("ìž¥ë¥´ :", "").strip()
            if genre_text:
                genre = genre_text.split(",")[0].strip()
                return genre

    return "ê¸°íƒ€"

def extract_runtime(soup: BeautifulSoup) -> int:
    spec_block = soup.select_one("div.spec")
    if not spec_block:
        return 0

    dt_elements = spec_block.select("dt")
    dd_elements = spec_block.select("dd")

    for i, dt in enumerate(dt_elements):
        if "ê¸°ë³¸ ì •ë³´" in dt.text:
            dd_text = dd_elements[i].text.strip()
            match = re.search(r"(\d+)\s*ë¶„", dd_text)
            if match:
                return int(match.group(1))
    return 0


class CGVCrawler(AbstractCrawlingService):

    def __init__(self, config):
        super().__init__(config)
        self.driver = create_driver()

    def get_crawling_data(self) -> ResultSet[Tag]:
        try:
            url = self.config["url"]
            self.driver.get(url)
            scroll_until_loaded(self.driver)
            html = self.driver.page_source
            soup = BeautifulSoup(html, "html.parser")
            return soup.select("div.mm_list_item")
        finally:
            self.driver.quit()

    def create_dto(self, element: Tag) -> dict:
        try:
            title_tag = element.select_one("div.tit_area strong.tit")
            if not title_tag:
                return {}

            #ì œëª©
            title = title_tag.text.strip()
            detail_url = extract_detail_url(element)

            detail_soup = get_detail_data(detail_url) if detail_url else None

            # ê°œë´‰ì¼
            raw_date = element.select_one("span.rel-date").text.strip() if element.select_one("span.rel-date") else ""
            release_date = raw_date.replace("ê°œë´‰", "").strip()
            opening_time = converter.string_to_epoch(release_date) if release_date else 0
            running_time = extract_runtime(detail_soup) if detail_soup else 0

            # í¬ìŠ¤í„°
            img_tag = element.select_one("span.imgbox img")
            poster = img_tag["src"] if img_tag else ""

            # ì¤„ê±°ë¦¬
            plot = "ì •ë³´ì—†ìŒ"
            if detail_soup:
                plot_tag = detail_soup.find("meta", attrs={"property": "og:description"})
                if plot_tag:
                    plot = plot_tag.get("content", "").strip()

            genre = extract_genre(detail_soup) if detail_soup else "ê¸°íƒ€"
            # ìž¥ë¥´ â†’ ì¹´í…Œê³ ë¦¬
            category_level_two = fetch_or_create_category(genre, "MOVIE")

            # ê°ë…, ë°°ìš°
            directors, actors = extract_director_and_actors(detail_soup) if detail_soup else ([], [])

            # ðŸ” KOFIC ì¸ë±ìŠ¤ ì¡°íšŒ (ë¬¸ìžì—´ë¡œ)
            kofic_index = search_kofic_index_by_title_and_director(title, directors)

            # ì˜ˆë§¤ ë§í¬
            reservation_link = [None, None, None]  # MEGA BOX, CGV, LOTTE
            if detail_url:
                reservation_link[1] = "https://www.cgv.co.kr/movies/detail-view/?midx=" + detail_url.split("=")[-1]

            if kofic_index:

                kofic_category = kofic_index.get("categoryLevelTwo", category_level_two)
                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ í•­ëª© ì‚¬ìš©
                if isinstance(kofic_category, list) and kofic_category:
                    kofic_category = kofic_category[0]

                is_update = exists_movie_by_kofic_code(kofic_index.get("KOFICCode"))
                # KOFIC ê¸°ë°˜ ì •ë³´ ë®ì–´ì“°ê¸°
                return {
                    "movieNm": kofic_index.get("movieNm", title),
                    "openingTime": kofic_index.get("openingTime", opening_time),
                    "KOFICCode": kofic_index.get("KOFICCode"),
                    "reservationLink": reservation_link,
                    "posterBase64": poster,
                    "directors": kofic_index.get("directors", []),
                    "actors": kofic_index.get("actors", []),
                    "companyNm": kofic_index.get("companyNm", []),
                    "categoryLevelOne": "MOVIE",
                    "categoryLevelTwo": kofic_category,
                    "runningTime": kofic_index.get("runningTime", 0),
                    "plot": plot if plot else "ì •ë³´ì—†ìŒ",
                    "favorites" : "",
                    "__update__": is_update
                }

            # fallback: CGV-only ì •ë³´ ê¸°ë°˜
            return {
                "movieNm": title,
                "openingTime": opening_time,
                "KOFICCode": "",
                "reservationLink": reservation_link,
                "posterBase64": poster,
                "directors" : directors,
                "actors" : actors,
                "companyNm" : "",
                "categoryLevelOne": "MOVIE",
                "categoryLevelTwo": category_level_two,
                "runningTime" : running_time,
                "plot": plot if plot else "ì •ë³´ì—†ìŒ",
                "favorites" : ""
            }

        except Exception as e:
            logger.warning(f"[CGV] DTO ìƒì„± ì‹¤íŒ¨: {e}")
            return {}

    def crawl(self) -> List[dict]:
        raw = self.get_crawling_data()
        results = [self.create_dto(item) for item in raw]
        logger.info(f"[CGV] Crawled {len(results)} items")
        return results